{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e39063",
   "metadata": {},
   "source": [
    "# ADS 509 Module 3: Group Comparison \n",
    "\n",
    "The task of comparing two groups of text is fundamental to textual analysis. There are innumerable applications: survey respondents from different segments of customers, speeches by different political parties, words used in Tweets by different constituencies, etc. In this assignment you will build code to effect comparisons between groups of text data, using the ideas learned in reading and lecture.\n",
    "\n",
    "This assignment asks you to analyze the lyrics for the two artists you selected in Module 1 and the Twitter descriptions pulled for Robyn and Cher. If the results from that pull were not to your liking, you are welcome to use the zipped data from the “Assignment Materials” section. Specifically, you are asked to do the following: \n",
    "\n",
    "* Read in the data, normalize the text, and tokenize it. When you tokenize your Twitter descriptions, keep hashtags and emojis in your token set. \n",
    "* Calculate descriptive statistics on the two sets of lyrics and compare the results. \n",
    "* For each of the four corpora, find the words that are unique to that corpus. \n",
    "* Build word clouds for all four corpora. \n",
    "\n",
    "Each one of the analyses has a section dedicated to it below. Before beginning the analysis there is a section for you to read in the data and do your cleaning (tokenization and normalization). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abe420bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from wordcloud import WordCloud \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9f064bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this space for any additional import statements you need\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcbe6342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place any addtional functions or constants you need here. \n",
    "\n",
    "# Some punctuation variations\n",
    "punctuation = set(punctuation) # speeds up comparison\n",
    "tw_punct = punctuation - {\"#\"}\n",
    "\n",
    "# Stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "# Two useful regex\n",
    "whitespace_pattern = re.compile(r\"\\s+\")\n",
    "hashtag_pattern = re.compile(r\"^#[0-9a-zA-Z]+\")\n",
    "\n",
    "# It's handy to have a full set of emojis\n",
    "all_language_emojis = set()\n",
    "\n",
    "for country in emoji.EMOJI_DATA : \n",
    "    for em in emoji.EMOJI_DATA[country] : \n",
    "        all_language_emojis.add(em)\n",
    "\n",
    "# and now our functions\n",
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity, and num_tokens most common\n",
    "        tokens. Return a list of \n",
    "    \"\"\"\n",
    "\n",
    "    # Fill in the correct values here. \n",
    "    num_tokens = len(tokens)\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    lexical_diversity = num_unique_tokens / num_tokens\n",
    "    num_characters = sum(len(token) for token in tokens)\n",
    "    \n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # Print the five most common tokens\n",
    "        top_five = Counter(tokens).most_common(5)\n",
    "        print(top_five)\n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])\n",
    "\n",
    "\n",
    "    \n",
    "def contains_emoji(s):\n",
    "    \n",
    "    s = str(s)\n",
    "    emojis = [ch for ch in s if emoji.is_emoji(ch)]\n",
    "\n",
    "    return(len(emojis) > 0)\n",
    "\n",
    "\n",
    "def remove_stop(tokens) :\n",
    "    \n",
    "    tokens = [word for word in tokens if word not in sw]\n",
    "    \n",
    "    return(tokens)\n",
    " \n",
    "def remove_punctuation(text, punct_set=tw_punct) : \n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "\n",
    "def tokenize(text) : \n",
    "    \"\"\" Splitting on whitespace rather than the book's tokenize function. That \n",
    "        function will drop tokens like '#hashtag' or '2A', which we need for Twitter. \"\"\"\n",
    "    \n",
    "    return text.split()\n",
    "\n",
    "def prepare(text, pipeline) : \n",
    "    tokens = str(text)\n",
    "    \n",
    "    for transform in pipeline : \n",
    "        tokens = transform(tokens)\n",
    "        \n",
    "    return(tokens)\n",
    "\n",
    "# Function to remove the song title from the lyrics\n",
    "def remove_title(text):\n",
    "   \n",
    "    # Song title is found before the first \\n\n",
    "    lyrics = text.split('\\n', 1)[1]\n",
    "            \n",
    "    return lyrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47735524",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Use this section to ingest your data into the data structures you plan to use. Typically this will be a dictionary or a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f371b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data location\n",
    "data_location = \"/Users/clairebentzen/Desktop/MDAS/ADS 509 - Applied Text Mining/Module 2/Assignment2.1/M1 Results/\"\n",
    "\n",
    "# Subfolders data from the Module 1 assignment\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\"\n",
    "\n",
    "# Specify artist_files for twitter data\n",
    "artist_files = {'cher':'cher_followers_data.txt',\n",
    "                'robyn':'robynkonichiwa_followers_data.txt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df415d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read cher twitter data\n",
    "twitter_data = pd.read_csv(data_location + twitter_folder + artist_files['cher'],\n",
    "                           sep=\"\\t\",\n",
    "                           quoting=3)\n",
    "\n",
    "twitter_data['artist'] = \"cher\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "966804cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read robyn twitter \n",
    "twitter_data_2 = pd.read_csv(data_location + twitter_folder + artist_files['robyn'],\n",
    "                             sep=\"\\t\",\n",
    "                             quoting=3)\n",
    "twitter_data_2['artist'] = \"robyn\"\n",
    "\n",
    "# Concat twitter dataframes\n",
    "twitter_data = pd.concat([\n",
    "    twitter_data,twitter_data_2])\n",
    "    \n",
    "del(twitter_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "674767d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the lyrics data\n",
    "# Specify pathway to lyrics folder\n",
    "lyrics_path = data_location + lyrics_folder\n",
    "\n",
    "# Create a dataframe to store results\n",
    "lyrics_data = pd.DataFrame(columns=['artist', 'song', 'lyrics'])\n",
    "\n",
    "# Iterate through each file in the lyrics folder\n",
    "for artist in os.listdir(lyrics_path):\n",
    "    artist_path = os.path.join(lyrics_path, artist)\n",
    "    \n",
    "    # Iterate through each file in the artist folders\n",
    "    for song in os.listdir(artist_path):\n",
    "        song_path = os.path.join(artist_path, song)\n",
    "        rem_prefix = song.removeprefix(f'{artist}_')\n",
    "        song_title = rem_prefix.removesuffix('.txt')\n",
    "\n",
    "        # Open and read the contents of the file (song)\n",
    "        with open(song_path, 'r') as file:\n",
    "            contents = file.read()\n",
    "            # Prepare data to add to dataframe\n",
    "            data = {'artist': artist, 'song': song_title, 'lyrics': contents}\n",
    "            # The df.append() function is deprecated, so we will ignore warnings here\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                # Append row of data to lyrics_df\n",
    "                lyrics_data = lyrics_data.append(data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9892d14",
   "metadata": {},
   "source": [
    "## Tokenization and Normalization\n",
    "\n",
    "In this next section, tokenize and normalize your data. We recommend the following cleaning. \n",
    "\n",
    "**Lyrics** \n",
    "\n",
    "* Remove song titles\n",
    "* Casefold to lowercase\n",
    "* Remove stopwords (optional)\n",
    "* Remove punctuation\n",
    "* Split on whitespace\n",
    "\n",
    "Removal of stopwords is up to you. Your descriptive statistic comparison will be different if you include stopwords, though TF-IDF should still find interesting features for you. Note that we remove stopwords before removing punctuation because the stopword set includes punctuation.\n",
    "\n",
    "**Twitter Descriptions** \n",
    "\n",
    "* Casefold to lowercase\n",
    "* Remove stopwords\n",
    "* Remove punctuation other than emojis or hashtags\n",
    "* Split on whitespace\n",
    "\n",
    "Removing stopwords seems sensible for the Twitter description data. Remember to leave in emojis and hashtags, since you analyze those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ca379eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pipeline to remove title, casefold, remove sw, remove punctuation, and split lyrics\n",
    "lyrics_pipeline = [remove_title, str.lower, remove_punctuation, tokenize, remove_stop]\n",
    "\n",
    "# Initialize pipeline to casefold, remove sw, remove punctuation, and split twitter descriptions\n",
    "twitter_pipeline = [str.lower, remove_punctuation, tokenize, remove_stop]\n",
    "\n",
    "# Apply pipeline to lyrics\n",
    "lyrics_data[\"tokens\"] = lyrics_data[\"lyrics\"].apply(prepare, pipeline=lyrics_pipeline)\n",
    "# Calculate number of tokens in each row\n",
    "lyrics_data[\"num_tokens\"] = lyrics_data[\"tokens\"].map(len) \n",
    "\n",
    "# Apply pipeline to twitter descriptions\n",
    "twitter_data[\"tokens\"] = twitter_data[\"description\"].apply(prepare,pipeline=twitter_pipeline)\n",
    "# Calculate number of tokens in each row\n",
    "twitter_data[\"num_tokens\"] = twitter_data[\"tokens\"].map(len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6cf534be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which twitter descriptions have emojis\n",
    "twitter_data['has_emoji'] = twitter_data[\"description\"].apply(contains_emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec69ac9",
   "metadata": {},
   "source": [
    "Let's take a quick look at some descriptions with emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a5a0512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>description</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1277283</th>\n",
       "      <td>cher</td>\n",
       "      <td>Mom who is sick of all these damn racists/bigo...</td>\n",
       "      <td>[mom, sick, damn, racistsbigotsxenophobes, bul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312403</th>\n",
       "      <td>cher</td>\n",
       "      <td>I am low key a living meme 😃</td>\n",
       "      <td>[low, key, living, meme, 😃]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284757</th>\n",
       "      <td>cher</td>\n",
       "      <td>World citizen. Kind of shy here. ✌️</td>\n",
       "      <td>[world, citizen, kind, shy, ✌️]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1475074</th>\n",
       "      <td>cher</td>\n",
       "      <td>I ❤my family, my little dogs. I like to bake f...</td>\n",
       "      <td>[❤my, family, little, dogs, like, bake, scratc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315427</th>\n",
       "      <td>cher</td>\n",
       "      <td>maybe just hit 50 but age, mheeee 🤷🏻‍♂️, educa...</td>\n",
       "      <td>[maybe, hit, 50, age, mheeee, 🤷🏻‍♂️, educates,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24391</th>\n",
       "      <td>cher</td>\n",
       "      <td>🇵🇭 l 📚 Epistemophile l 🎥 Primera Cinema Produc...</td>\n",
       "      <td>[🇵🇭, l, 📚, epistemophile, l, 🎥, primera, cinem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098649</th>\n",
       "      <td>cher</td>\n",
       "      <td>just here to fulfill my fantasies 😋</td>\n",
       "      <td>[fulfill, fantasies, 😋]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263416</th>\n",
       "      <td>cher</td>\n",
       "      <td>💚💛❤️</td>\n",
       "      <td>[💚💛❤️]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188180</th>\n",
       "      <td>robyn</td>\n",
       "      <td>♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ...</td>\n",
       "      <td>[♥, ♥, ♥, ♥, ♥, ♥, ♥, ♥, ♥, ♥, ♥, ♥, ♥, ♥, ♥, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2109999</th>\n",
       "      <td>cher</td>\n",
       "      <td>mostly high thots, LPT 🦋she/her</td>\n",
       "      <td>[mostly, high, thots, lpt, 🦋sheher]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        artist                                        description  \\\n",
       "1277283   cher  Mom who is sick of all these damn racists/bigo...   \n",
       "312403    cher                       I am low key a living meme 😃   \n",
       "1284757   cher                World citizen. Kind of shy here. ✌️   \n",
       "1475074   cher  I ❤my family, my little dogs. I like to bake f...   \n",
       "1315427   cher  maybe just hit 50 but age, mheeee 🤷🏻‍♂️, educa...   \n",
       "24391     cher  🇵🇭 l 📚 Epistemophile l 🎥 Primera Cinema Produc...   \n",
       "1098649   cher                just here to fulfill my fantasies 😋   \n",
       "263416    cher                                               💚💛❤️   \n",
       "188180   robyn  ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ♥ ...   \n",
       "2109999   cher                    mostly high thots, LPT 🦋she/her   \n",
       "\n",
       "                                                    tokens  \n",
       "1277283  [mom, sick, damn, racistsbigotsxenophobes, bul...  \n",
       "312403                         [low, key, living, meme, 😃]  \n",
       "1284757                    [world, citizen, kind, shy, ✌️]  \n",
       "1475074  [❤my, family, little, dogs, like, bake, scratc...  \n",
       "1315427  [maybe, hit, 50, age, mheeee, 🤷🏻‍♂️, educates,...  \n",
       "24391    [🇵🇭, l, 📚, epistemophile, l, 🎥, primera, cinem...  \n",
       "1098649                            [fulfill, fantasies, 😋]  \n",
       "263416                                              [💚💛❤️]  \n",
       "188180   [♥, ♥, ♥, ♥, ♥, ♥, ♥, ♥, ♥, ♥, ♥, ♥, ♥, ♥, ♥, ...  \n",
       "2109999                [mostly, high, thots, lpt, 🦋sheher]  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data[twitter_data.has_emoji].sample(10)[[\"artist\",\"description\",\"tokens\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2c55c9",
   "metadata": {},
   "source": [
    "With the data processed, we can now start work on the assignment questions. \n",
    "\n",
    "Q: What is one area of improvement to your tokenization that you could theoretically carry out? (No need to actually do it; let's not make perfect the enemy of good enough.)\n",
    "\n",
    "A: One area of improvement I could perform on my tokenization has to do with the emojis. It appears that the emojis are only tokenized individually if there are spaces between them. If an emoji is next to another emoji or other character, then the emoji will not be a separate token. Ideally, each emoji should be its own token regardless of it there is a space next to it or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1594271",
   "metadata": {},
   "source": [
    "## Calculate descriptive statistics on the two sets of lyrics and compare the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc25e937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 35233 tokens in the data.\n",
      "There are 3684 unique tokens in the data.\n",
      "There are 169244 characters in the data.\n",
      "The lexical diversity is 0.105 in the data.\n",
      "[('love', 966), ('im', 511), ('know', 480), ('dont', 430), ('youre', 332)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bj/4_046k_n1259zwtr2hcbxcbw0000gn/T/ipykernel_94152/4186478837.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cher_lyrics['cleaned_lyrics'] = cher_lyrics['tokens'].apply(lambda x: ' '.join(x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[35233, 3684, 0.10456106491073709, 169244]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cher Lyrics Stats\n",
    "# Subset cher lyrics\n",
    "cher_lyrics = lyrics_data[lyrics_data['artist'] == 'cher']\n",
    "# Concat cleaned lyrics into one string\n",
    "cher_lyrics['cleaned_lyrics'] = cher_lyrics['tokens'].apply(lambda x: ' '.join(x))\n",
    "cher_lyrics_str = cher_lyrics['cleaned_lyrics'].str.cat(sep=' ')\n",
    "\n",
    "# Calculate descriptive stats\n",
    "cher_lyrics_desc = descriptive_stats(cher_lyrics_str.split())\n",
    "cher_lyrics_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae30d4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15041 tokens in the data.\n",
      "There are 2139 unique tokens in the data.\n",
      "There are 72804 characters in the data.\n",
      "The lexical diversity is 0.142 in the data.\n",
      "[('know', 305), ('im', 299), ('dont', 297), ('love', 269), ('got', 249)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bj/4_046k_n1259zwtr2hcbxcbw0000gn/T/ipykernel_94152/2043651631.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  robyn_lyrics['cleaned_lyrics'] = robyn_lyrics['tokens'].apply(lambda x: ' '.join(x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[15041, 2139, 0.1422112891430091, 72804]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Robyn Lyrics Stats\n",
    "# Subset robyn lyrics\n",
    "robyn_lyrics = lyrics_data[lyrics_data['artist'] == 'robyn']\n",
    "# Concat cleaned lyrics into one string\n",
    "robyn_lyrics['cleaned_lyrics'] = robyn_lyrics['tokens'].apply(lambda x: ' '.join(x))\n",
    "robyn_lyrics_str = robyn_lyrics['cleaned_lyrics'].str.cat(sep=' ')\n",
    "\n",
    "# Calculate descriptive stats\n",
    "robyn_lyrics_desc = descriptive_stats(robyn_lyrics_str.split())\n",
    "robyn_lyrics_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2ada9",
   "metadata": {},
   "source": [
    "Q: what observations do you make about these data? \n",
    "\n",
    "A: The Cher lyrics contain more tokens, unique tokens, and characters, but the lexical diversity is slighly lower than the Robyn lyrics. An interesting observation is that out of the top 5 most common words, these two artists share 4 of them, although they are in different orders. These 4 words they have in common are love, know, im, and dont."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750aa526",
   "metadata": {},
   "source": [
    "## Find tokens uniquely related to a corpus\n",
    "\n",
    "Typically we would use TF-IDF to find unique tokens in documents. Unfortunately, we either have too few documents (if we view each data source as a single document) or too many (if we view each description as a separate document). In the latter case, our problem will be that descriptions tend to be short, so our matrix would be too sparse to support analysis. \n",
    "\n",
    "To avoid these problems, we will create a custom statistic to identify words that are uniquely related to each corpus. The idea is to find words that occur often in one corpus and infrequently in the other(s). Since corpora can be of different lengths, we will focus on the _concentration_ of tokens within a corpus. \"Concentration\" is simply the count of the token divided by the total corpus length. For instance, if a corpus had length 100,000 and a word appeared 1,000 times, then the concentration would be $\\frac{1000}{100000} = 0.01$. If the same token had a concentration of $0.005$ in another corpus, then the concentration ratio would be $\\frac{0.01}{0.005} = 2$. Very rare words can easily create infinite ratios, so you will also add a cutoff to your code so that a token must appear at least $n$ times for you to return it. \n",
    "\n",
    "An example of these calculations can be found in [this spreadsheet](https://docs.google.com/spreadsheets/d/1P87fkyslJhqXFnfYezNYrDrXp_GS8gwSATsZymv-9ms). Please don't hesitate to ask questions if this is confusing. \n",
    "\n",
    "In this section find 10 tokens for each of your four corpora that meet the following criteria: \n",
    "\n",
    "1. The token appears at least `n` times in all corpora\n",
    "1. The tokens are in the top 10 for the highest ratio of appearances in a given corpora vs appearances in other corpora.\n",
    "\n",
    "You will choose a cutoff for yourself based on the side of the corpus you're working with. If you're working with the Robyn-Cher corpora provided, `n=5` seems to perform reasonably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce72f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53526fcd",
   "metadata": {},
   "source": [
    "Q: What are some observations about the top tokens? Do you notice any interesting items on the list? \n",
    "\n",
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f52b3",
   "metadata": {},
   "source": [
    "## Build word clouds for all four corpora. \n",
    "\n",
    "For building wordclouds, we'll follow exactly the code of the text. The code in this section can be found [here](https://github.com/blueprints-for-text-analytics-python/blueprints-text/blob/master/ch01/First_Insights.ipynb). If you haven't already, you should absolutely clone the repository that accompanies the book. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def wordcloud(word_freq, title=None, max_words=200, stopwords=None):\n",
    "\n",
    "    wc = WordCloud(width=800, height=400, \n",
    "                   background_color= \"black\", colormap=\"Paired\", \n",
    "                   max_font_size=150, max_words=max_words)\n",
    "    \n",
    "    # convert data frame into dict\n",
    "    if type(word_freq) == pd.Series:\n",
    "        counter = Counter(word_freq.fillna(0).to_dict())\n",
    "    else:\n",
    "        counter = word_freq\n",
    "\n",
    "    # filter stop words in frequency counter\n",
    "    if stopwords is not None:\n",
    "        counter = {token:freq for (token, freq) in counter.items() \n",
    "                              if token not in stopwords}\n",
    "    wc.generate_from_frequencies(counter)\n",
    " \n",
    "    plt.title(title) \n",
    "\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    \n",
    "def count_words(df, column='tokens', preprocess=None, min_freq=2):\n",
    "\n",
    "    # process tokens and update counter\n",
    "    def update(doc):\n",
    "        tokens = doc if preprocess is None else preprocess(doc)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # create counter and run through all data\n",
    "    counter = Counter()\n",
    "    df[column].map(update)\n",
    "\n",
    "    # transform counter into data frame\n",
    "    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "    freq_df = freq_df.query('freq >= @min_freq')\n",
    "    freq_df.index.name = 'token'\n",
    "    \n",
    "    return freq_df.sort_values('freq', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a2e53",
   "metadata": {},
   "source": [
    "Q: What observations do you have about these (relatively straightforward) wordclouds? \n",
    "\n",
    "A: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
